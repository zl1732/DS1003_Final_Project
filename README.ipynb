{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation Knowledge Sources\n",
    "To train an all-word WSD model, several knowledge sources are neccesary. Including :\n",
    "- Sense inventory\n",
    "- Sense labeled corpus\n",
    "- Embeddings\n",
    "\n",
    "Here we explore how to leverage these knowledge sources using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense inventory word-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The powerful NLTK package includes handy wordnet interface. The senses of a word is stored as form of synset, which includes the definition of a sense and words related to this sense.\n",
    "\n",
    "For a more detailed documentation of wordnet interface, please refer to : http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For reading and documentation about NLTK package, please read:\n",
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to all the synsets of a particular word, use wn.synsets(word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('qualify.v.01'),\n",
       " Synset('qualify.v.02'),\n",
       " Synset('qualify.v.03'),\n",
       " Synset('qualify.v.04'),\n",
       " Synset('stipulate.v.01'),\n",
       " Synset('qualify.v.06'),\n",
       " Synset('modify.v.02')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example. \n",
    "wn.synsets('qualify')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under every synnet, it contains the lemmas that are  in this synnet. use .lemmas() to access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qualify_.v.03 definition: make more specific\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Lemma('qualify.v.03.qualify'), Lemma('qualify.v.03.restrict')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Let's choose the third synnet of the word 'qualify', which is not its usual sense.\n",
    "qualify_03 = wn.synsets('qualify')[2]\n",
    "# It is also easy to print out its definition\n",
    "print('qualify_.v.03 definition: %s'%(qualify_03.definition()))\n",
    "qualify_03.lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense labeled corpus\n",
    "NLTK also contains wordnet's senses labeled corpus: SemCor. For a detail documentation of NTLK tagged corpora please refer to:\n",
    "http://www.nltk.org/howto/corpus.html#tagged-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SemCor corpus is a chunk corpus that would group words in a sentence into chunks. The senses taged are associated with each chunk (usually nouns phrases will be chunked) instead of word (One can see how bad word level definition could be). To access to all chunks simply use .tagged_chunks(). One can access to either or both POS and sense tag of each chunk by specifying tag='pos' tag='sem' or tag='both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunks = semcor.tagged_chunks(tag='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = semcor.tagged_sents(tag='sem')[0]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful methods of chunk class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_chunk= chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fulton', 'County', 'Grand', 'Jury']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing words in a chunk\n",
    "example_chunk.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('group.n.01.group')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing the lemma of a chunk.\n",
    "example_chunk.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('group.n.01')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing the synset of a chunk\n",
    "example_chunk.label().synset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one is interested in the pure sentence instead of its labels and syntatic tree. We can use .sents() method to access to tokenized centences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = semcor.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'group.n.01'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_synset = wn.synsets('group')[0]\n",
    "example_synset.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two useful pretrained embeddings are: Word2Vec and Glove. The following functions are used to extract word embeddings from GloVe dataset.\n",
    "load_glove_embeddings(glove_directory,emsize,voc_size) it takes in:\n",
    "\n",
    "- glove_directory: the directory(default glove.6B) you save your glove embeddings \n",
    "- emsize: the embedding size of your glove embeddings must be one of 50,100 amd 300\n",
    "- voc_size: number of vocabulary you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "def load_glove_embeddings(glove_directory,emsize=50,voc_size=50000):\n",
    "    #get directory name glove.6B or other training corpus size\n",
    "    if glove_directory[-1] =='/':\n",
    "        dirname = glove_directory.split('/')[-2]\n",
    "    else:\n",
    "        dirname = glove_directory.split('/')[-1]\n",
    "    if emsize in [50,100,300]:\n",
    "        f = open(os.path.join(glove_directory,'%s.%sd.txt'%(dirname,emsize)))\n",
    "    else:\n",
    "        print('Please select from 50, 100 or 300')\n",
    "        return\n",
    "    loaded_embeddings = collections.defaultdict()\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= voc_size: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[s[0]] = np.asarray(s[1:],dtype='float64')\n",
    "    return loaded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: GloVe Extraction\n",
    "loaded_embeddings = load_glove_embeddings('../datasets/glove.6B/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66385   0.41015   0.073617  0.85937   0.30031  -0.11978  -0.45367\n",
      "  1.4574   -0.73222   0.28086  -0.7589   -1.2996   -0.96887  -0.57294\n",
      " -0.25255  -0.7098    0.52366  -1.3184   -1.7125   -0.074232 -1.2343\n",
      " -0.37677  -0.4526   -0.95694   0.36827  -1.8201   -0.20622  -0.31884\n",
      "  0.1527   -0.30461   2.3935    1.3234   -1.0144    0.35188  -0.17079\n",
      " -0.67128   0.38904   0.94105  -0.42382  -1.3848    0.15837  -0.59283\n",
      " -0.80945  -0.46636  -0.086871  1.419    -0.5528   -0.19525   0.43202\n",
      " -0.6991  ]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_embeddings['victory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save embeddings for future use\n",
    "import pickle\n",
    "with open('glove_50d_50kvoc.pk', 'wb') as fp:\n",
    "    pickle.dump(loaded_embeddings, fp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
