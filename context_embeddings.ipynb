{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Sense Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sense embeddings of a certain sense is calculated by averaging the context embeddings of all context in which certain sense exists. There exists several different methods for combining words embeddings to form context embeddings. Our starting poing is applying plain average (bag of word). \n",
    "\n",
    "Reference: Iaacobaci et al, Embeddings for Word Sense Disambiguation: An Evaluation Study\n",
    "http://aclweb.org/anthology/P/P16/P16-1085.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load an example embeddings\n",
    "embedding_dict = pickle.load(open('glove_50d_50kvoc.pk','rb'))\n",
    "example_sentence = semcor.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_chunk = semcor.tagged_sents(tag='sem')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence_list = semcor.tagged_sents(tag='sem')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a function to combine word to form context embedding:\n",
    "def getContextEmb(sentence,center,window_size,embedding_dict,emb_size):\n",
    "    # Input introductions\n",
    "    # sentence: an array of tokens of untagged sentence. \n",
    "    # center: position of the center word\n",
    "    # window_size: size of context window\n",
    "    # embedding_Dict: embedding dictionary used to calculate context\n",
    "    ################################################################\n",
    "    start_pos = max([0,center-window_size])\n",
    "    end_pos = min([len(sentence),(center+window_size)+1])\n",
    "    context_tokens = sentence[start_pos:end_pos]\n",
    "    output_embedding = np.zeros(emb_size)\n",
    "    for word in context_tokens:\n",
    "        try:\n",
    "            output_embedding+=embedding_dict[word]\n",
    "        except:\n",
    "            output_embedding+=np.random.uniform(1,-1,emb_size)\n",
    "    return output_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to create a method to form a dictionary of sense embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildSemEmb(tagged_sents,emb_size,embedding_dict,context_builder = getContextEmb):\n",
    "    output_dict = collections.defaultdict(lambda: np.zeros(emb_size))\n",
    "    count_dict = collections.defaultdict(lambda: 0)\n",
    "    for sentence in tagged_sents:\n",
    "        #print(sentence)\n",
    "        for idx,chunk in enumerate(sentence):\n",
    "            if(type(chunk))==list:\n",
    "                continue\n",
    "            else:\n",
    "                #Use try except handling since some of the label is broken\n",
    "                try:\n",
    "                    sense_index = chunk.label().synset().name()\n",
    "                except:\n",
    "                    continue\n",
    "                context_emb = context_builder(sentence,idx,3,embedding_dict,emb_size)\n",
    "                output_dict[sense_index]+=context_emb\n",
    "                count_dict[sense_index]+=1\n",
    "    # Averaging\n",
    "    for key in output_dict.keys():\n",
    "        output_dict[key] /= count_dict[key]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a sense embedding dictionary for prediction. Notice that the ouput dictionary of buildSemEmb() is a collection.defaultdict() with default value being the uniform random vector. Hence it returns a uniform random vector when some sense does not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build sense dictionary for semcor corpus\n",
    "semcor_senseEmb = buildSemEmb(semcor.tagged_sents(tag='sem'),50,embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.30003492, -0.09053706, -0.22600043,  1.06458565, -1.23842165,\n",
       "       -2.46055579, -3.43967801, -3.36920484,  0.13196153, -2.7397281 ,\n",
       "        2.89399486,  1.65822149,  2.10908488, -0.95701491, -1.44100407,\n",
       "       -0.11627587,  1.65703584,  0.85361903,  2.04998585, -0.19486962,\n",
       "        0.48201897, -2.1837728 , -0.83654919, -0.47885907, -1.16298954,\n",
       "       -1.42235606,  0.90693904, -0.85483815,  1.06258396, -1.00788814,\n",
       "        0.41451083, -1.4589622 , -0.9678323 ,  1.92015602, -1.31059001,\n",
       "        2.3856872 ,  0.21004829, -0.03023176, -2.02274583, -1.1966199 ,\n",
       "       -1.82946856,  1.2398847 , -1.77989179,  0.68169361, -0.47411504,\n",
       "       -1.97803033, -0.10702167,  2.10165498,  3.00362617, -2.48403455])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_senseEmb['commitment.n.03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expriment: bag of word comparison with sense embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a trained embeddings and the sense embeddings that we derived by averaging the context. We can build a classifier that directly compare the bag of words (the average embeddings of the entire sentence) with sense embeddings and output the sense with highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The'],\n",
       " Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]),\n",
       " Tree(Lemma('state.v.01.say'), ['said']),\n",
       " Tree(Lemma('friday.n.01.Friday'), ['Friday']),\n",
       " ['an'],\n",
       " Tree(Lemma('probe.n.01.investigation'), ['investigation']),\n",
       " ['of'],\n",
       " Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']),\n",
       " [\"'s\"],\n",
       " Tree(Lemma('late.s.03.recent'), ['recent']),\n",
       " Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']),\n",
       " Tree(Lemma('produce.v.04.produce'), ['produced']),\n",
       " ['``'],\n",
       " ['no'],\n",
       " Tree(Lemma('evidence.n.01.evidence'), ['evidence']),\n",
       " [\"''\"],\n",
       " ['that'],\n",
       " ['any'],\n",
       " Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']),\n",
       " Tree(Lemma('happen.v.01.take_place'), ['took', 'place']),\n",
       " ['.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('produce.v.01', 0.79938981141141463),\n",
       " ('produce.v.02', 0.95091995923310568),\n",
       " ('produce.v.03', 1.1212183515751915),\n",
       " ('produce.v.04', 0.9652418620686104),\n",
       " ('grow.v.07', 0.95999618009120324),\n",
       " ('produce.v.06', 0.86363817985361668),\n",
       " ('grow.v.08', 1.2094539090980427)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_word = 'primary'\n",
    "example_context = getContextEmb(center=15,emb_size=50,embedding_dict=embedding_dict,sentence=example_sentence,window_size=2)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "choices = [synset.name() for synset in wn.synsets('produced')]\n",
    "\n",
    "decision_chart = [(choice,cosine(example_context,semcor_senseEmb[choice])) for choice in choices]\n",
    "\n",
    "decision_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bring forth or yield'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('produce')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bring out for display'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('produce')[4].definition()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
