{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Sense Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sense embeddings of a certain sense is calculated by averaging the context embeddings of all context in which certain sense exists. There exists several different methods for combining words embeddings to form context embeddings. Our starting poing is applying plain average (bag of word). \n",
    "\n",
    "Reference: Iaacobaci et al, Embeddings for Word Sense Disambiguation: An Evaluation Study\n",
    "http://aclweb.org/anthology/P/P16/P16-1085.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import progressbar\n",
    "\n",
    "with progressbar.ProgressBar(max_value=10) as bar:\n",
    "    for i in range(10):\n",
    "        bar.update(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import neccesary libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import semcor\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import dill\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './skipgram/model_winSize4_cpu.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd2543be785d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Load our trained embedding for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#need model.py in directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./skipgram/model_winSize4_cpu.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0memb_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0membedding_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_trained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhuorulin/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './skipgram/model_winSize4_cpu.pt'"
     ]
    }
   ],
   "source": [
    "# #Load an example embeddings\n",
    "# embedding_dict = pickle.load(open('glove_50d_50kvoc.pk','rb'))\n",
    "# example_sentence = semcor.sents()[0]\n",
    "\n",
    "#Load our trained embedding for test\n",
    "#need model.py in directory\n",
    "model_trained = torch.load('./skipgram/model_winSize4_cpu.pt')\n",
    "emb_trained = model_trained.encoder\n",
    "embedding_dict = emb_trained.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_chunk = semcor.tagged_sents(tag='sem')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence_list = semcor.tagged_sents(tag='sem')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a function to combine word to form context embedding:\n",
    "def getContextEmb(sentence,center,window_size,embedding_dict,emb_size):\n",
    "    # Input introductions\n",
    "    # sentence: an array of tokens of untagged sentence. \n",
    "    # center: position of the center word\n",
    "    # window_size: size of context window\n",
    "    # embedding_Dict: embedding dictionary used to calculate context\n",
    "    ################################################################\n",
    "    start_pos = max([0,center-window_size])\n",
    "    end_pos = min([len(sentence),(center+window_size)+1])\n",
    "    context_tokens = sentence[start_pos:end_pos]\n",
    "    output_embedding = np.zeros(emb_size)\n",
    "    for word in context_tokens:\n",
    "        try:\n",
    "            output_embedding+=embedding_dict[word]\n",
    "        except:\n",
    "            output_embedding+=np.random.uniform(1,-1,emb_size)\n",
    "    return output_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to create a method to form a dictionary of sense embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildSemEmb(tagged_sents,emb_size,embedding_dict,window_size=4):\n",
    "    def getContextEmb(sentence,center,window_size,embedding_dict,emb_size):\n",
    "    # Input introductions\n",
    "    # sentence: an array of tokens of untagged sentence. \n",
    "    # center: position of the center word\n",
    "    # window_size: size of context window\n",
    "    # embedding_Dict: embedding dictionary used to calculate context\n",
    "    ################################################################\n",
    "        start_pos = max([0,center-window_size])\n",
    "        end_pos = min([len(sentence),(center+window_size)+1])\n",
    "        context_tokens = sentence[start_pos:end_pos]\n",
    "        output_embedding = np.zeros(emb_size)\n",
    "        for word in context_tokens:\n",
    "            try:\n",
    "                output_embedding+=embedding_dict[word]\n",
    "            except:\n",
    "                output_embedding+=np.random.uniform(1,-1,emb_size)\n",
    "        return output_embedding\n",
    "\n",
    "    output_dict = collections.defaultdict(lambda: np.zeros(emb_size))\n",
    "    count_dict = collections.defaultdict(lambda: 0)\n",
    "    for sentence in tagged_sents:\n",
    "        #print(sentence)\n",
    "        for idx,chunk in enumerate(sentence):\n",
    "            if(type(chunk))==list:\n",
    "                continue\n",
    "            else:\n",
    "                #Use try except handling since some of the label is broken\n",
    "                try:\n",
    "                    sense_index = chunk.label().synset().name()\n",
    "                except:\n",
    "                    continue\n",
    "                context_emb = getContextEmb(sentence,idx,window_size,embedding_dict,emb_size)\n",
    "                output_dict[sense_index]+=context_emb\n",
    "                count_dict[sense_index]+=1\n",
    "    # Averaging\n",
    "    for key in output_dict.keys():\n",
    "        output_dict[key] /= count_dict[key]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a sense embedding dictionary for prediction. Notice that the ouput dictionary of buildSemEmb() is a collection.defaultdict() with default value being the uniform random vector. Hence it returns a uniform random vector when some sense does not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build sense dictionary for semcor corpus\n",
    "semcor_senseEmb = buildSemEmb(semcor.tagged_sents(tag='sem'),50,embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.30003492, -0.09053706, -0.22600043,  1.06458565, -1.23842165,\n",
       "       -2.46055579, -3.43967801, -3.36920484,  0.13196153, -2.7397281 ,\n",
       "        2.89399486,  1.65822149,  2.10908488, -0.95701491, -1.44100407,\n",
       "       -0.11627587,  1.65703584,  0.85361903,  2.04998585, -0.19486962,\n",
       "        0.48201897, -2.1837728 , -0.83654919, -0.47885907, -1.16298954,\n",
       "       -1.42235606,  0.90693904, -0.85483815,  1.06258396, -1.00788814,\n",
       "        0.41451083, -1.4589622 , -0.9678323 ,  1.92015602, -1.31059001,\n",
       "        2.3856872 ,  0.21004829, -0.03023176, -2.02274583, -1.1966199 ,\n",
       "       -1.82946856,  1.2398847 , -1.77989179,  0.68169361, -0.47411504,\n",
       "       -1.97803033, -0.10702167,  2.10165498,  3.00362617, -2.48403455])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_senseEmb['commitment.n.03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expriment: bag of word comparison with sense embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a trained embeddings and the sense embeddings that we derived by averaging the context. We can build a classifier that directly compare the bag of words (the average embeddings of the entire sentence) with sense embeddings and output the sense with highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The'],\n",
       " Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]),\n",
       " Tree(Lemma('state.v.01.say'), ['said']),\n",
       " Tree(Lemma('friday.n.01.Friday'), ['Friday']),\n",
       " ['an'],\n",
       " Tree(Lemma('probe.n.01.investigation'), ['investigation']),\n",
       " ['of'],\n",
       " Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']),\n",
       " [\"'s\"],\n",
       " Tree(Lemma('late.s.03.recent'), ['recent']),\n",
       " Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']),\n",
       " Tree(Lemma('produce.v.04.produce'), ['produced']),\n",
       " ['``'],\n",
       " ['no'],\n",
       " Tree(Lemma('evidence.n.01.evidence'), ['evidence']),\n",
       " [\"''\"],\n",
       " ['that'],\n",
       " ['any'],\n",
       " Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']),\n",
       " Tree(Lemma('happen.v.01.take_place'), ['took', 'place']),\n",
       " ['.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('produce.v.01', 0.79938981141141463),\n",
       " ('produce.v.02', 0.95091995923310568),\n",
       " ('produce.v.03', 1.1212183515751915),\n",
       " ('produce.v.04', 0.9652418620686104),\n",
       " ('grow.v.07', 0.95999618009120324),\n",
       " ('produce.v.06', 0.86363817985361668),\n",
       " ('grow.v.08', 1.2094539090980427)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_word = 'primary'\n",
    "example_context = getContextEmb(center=15,emb_size=50,embedding_dict=embedding_dict,sentence=example_sentence,window_size=2)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "choices = [synset.name() for synset in wn.synsets('produced')]\n",
    "\n",
    "decision_chart = [(choice,cosine(example_context,semcor_senseEmb[choice])) for choice in choices]\n",
    "\n",
    "decision_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bring forth or yield'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('produce')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bring out for display'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('produce')[4].definition()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
